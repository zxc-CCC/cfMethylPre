{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial1: Generate the PPE matrix and expand the methylation chip probe information.\n",
    "This tutorial provides an example of Generating PPE matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import pandas as pd\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPU found. Please make sure a GPU is available.\")\n",
    "    exit()\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_seq = pd.read_csv('../result/GNN_encode/probe_seq_all.csv')\n",
    "data = list(zip(probe_seq['IlmnID'], probe_seq['SourceSeq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model = model.to(device)\n",
    "model.eval() \n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_representations = []\n",
    "for i in range(0, len(data), batch_size):\n",
    "    batch_data = data[i:i+batch_size]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[48], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][48]\n",
    "\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0).tolist())\n",
    "\n",
    "columns = ['representation_{}'.format(i) for i in range(len(sequence_representations[0]))]\n",
    "\n",
    "result_df = pd.DataFrame(sequence_representations, columns=columns)\n",
    "result_df['IlmnID'] = probe_seq['IlmnID']\n",
    "result_df.to_csv('../data/encode_matrix/t48_15B_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
